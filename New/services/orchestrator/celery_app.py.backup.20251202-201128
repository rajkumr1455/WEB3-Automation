"""
Celery application and tasks for Web3 Hunter
Message broker: Redis
Result backend: Redis
"""
from celery import Celery
import os
import logging

logger = logging.getLogger(__name__)

# Create Celery app instance
celery_app = Celery(
    'web3hunter',
    broker=os.getenv('CELERY_BROKER_URL', 'redis://redis:6379/0'),
    backend=os.getenv('CELERY_RESULT_BACKEND', 'redis://redis:6379/1')
)

# Celery configuration
celery_app.conf.update(
    # Serialization
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    result_expires=3600,
    
    # Timezone
    timezone='UTC',
    enable_utc=True,
    
    # Task tracking
    task_track_started=True,
    task_send_sent_event=True,
    
    # Time limits
    task_time_limit=3600,  # 1 hour hard limit
    task_soft_time_limit=3300,  # 55 minute soft limit
    
    # Worker settings
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=50,
    
    # Task acknowledgement
    task_acks_late=True,
    task_reject_on_worker_lost=True,
)


# ============================================================================
# TASK DEFINITIONS
# ============================================================================

@celery_app.task(
    bind=True,
    name='orchestrator.run_scan',
    max_retries=3,
    default_retry_delay=60,
    autoretry_for=(Exception,),
    retry_backoff=True,
    retry_backoff_max=600,
    retry_jitter=True
)
def run_scan_task(self, scan_id: str, scan_request: dict):
    """
    Celery task to run security scan pipeline asynchronously
    
    Args:
        self: Celery task instance
        scan_id: Unique scan identifier
        scan_request: Scan request parameters as dict
    """
    try:
        logger.info(f"[Celery] Starting scan {scan_id}")
        
        # Import here to avoid circular dependencies
        from app import run_scan_pipeline_sync
        from models import ScanRequest as ScanRequestModel
        
        # Convert dict to Pydantic model
        request = ScanRequestModel(**scan_request)
        
        # Run the synchronous scan pipeline
        run_scan_pipeline_sync(scan_id, request)
        
        logger.info(f"[Celery] Scan {scan_id} completed successfully")
        
        return {
            "scan_id": scan_id,
            "status": "completed",
            "task_id": self.request.id
        }
        
    except Exception as exc:
        logger.error(f"[Celery] Scan {scan_id} failed (attempt {self.request.retries + 1}): {exc}")
        
        # Update scan status in database
        try:
            from db_helpers import update_scan_status
            update_scan_status(
                scan_id,
                "failed",
                error=f"Task failed: {str(exc)}"
            )
        except Exception as db_error:
            logger.error(f"[Celery] Failed to update scan status: {db_error}")
        
        # Celery will auto-retry
        raise


if __name__ == '__main__':
    celery_app.start()
